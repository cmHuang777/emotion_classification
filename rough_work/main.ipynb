{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "\n",
    "tweets = pd.read_csv(\"all_tweets.csv\")\n",
    "\n",
    "for i in range(len(tweets['text'])):\n",
    "    tweets.loc[i, 'text'] = re.sub(r'@\\S+', '@user', tweets['text'][i])\n",
    "\n",
    "tweets.to_csv('masked_all_tweets.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "def jsonl_to_csv(jsonl_file, csv_file):\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as infile, open(csv_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([\"speaker\", \"text\"])\n",
    "        \n",
    "        for i, line in enumerate(infile):\n",
    "            dialogue = json.loads(line.strip())\n",
    "            if not dialogue:\n",
    "                continue\n",
    "            writer.writerow([f\"Dialogue{i}\", \"\"])\n",
    "            speakers = {}\n",
    "            j = 0\n",
    "            for utterance in dialogue['conversation']:\n",
    "                user = utterance['speaker']\n",
    "                if user not in speakers:\n",
    "                    speakers[user] = j\n",
    "                    j += 1\n",
    "                \n",
    "                speaker = f\"Speaker_{speakers[user]}\"\n",
    "                text = utterance['utterance']\n",
    "                writer.writerow([speaker, text])\n",
    "\n",
    "# Specify the input JSONL file and the output CSV file\n",
    "jsonl_file = '500conv_global_reddit_with_index.jsonl'\n",
    "csv_file = '500conv_global_reddit_with_index.csv'\n",
    "\n",
    "# Convert JSONL to CSV\n",
    "jsonl_to_csv(jsonl_file, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 833\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import spacy\n",
    "\n",
    "'''\n",
    "Converst jsonl to csv file. Breaks up sentences in text and join them if not exceeding token limit.\n",
    "'''\n",
    "def jsonl_to_csv_with_token_limit(jsonl_file, csv_file, token_limit=100):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def split_into_sentences(text):\n",
    "        doc = nlp(text)\n",
    "        sentences = [sent.text for sent in doc.sents]\n",
    "        segments = []\n",
    "        cur_segment = \"\"\n",
    "        cur_len = 0\n",
    "        for sentence in sentences:\n",
    "            k = len(nlp(sentence)) \n",
    "            if k + cur_len <= token_limit:\n",
    "                cur_segment += sentence\n",
    "                cur_len += k\n",
    "            else:\n",
    "                cur_len = k\n",
    "                segments.append(cur_segment)\n",
    "                cur_segment = sentence\n",
    "        \n",
    "\n",
    "        if cur_segment:\n",
    "            # cur_segment = cur_segment[:-4]\n",
    "            segments.append(cur_segment)\n",
    "\n",
    "        return segments\n",
    "\n",
    "    num_of_rows = 0\n",
    "\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as infile, open(csv_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([\"speaker\", \"text\"])\n",
    "        \n",
    "        for i, line in enumerate(infile):\n",
    "            dialogue = json.loads(line.strip())\n",
    "            if not dialogue:\n",
    "                continue\n",
    "            writer.writerow([f\"Dialogue{i}\", \"\"])\n",
    "            speakers = {}\n",
    "            j = 0\n",
    "            for utterance in dialogue['conversation']:\n",
    "                user = utterance['speaker']\n",
    "                if user not in speakers:\n",
    "                    speakers[user] = j\n",
    "                    j += 1\n",
    "                \n",
    "                speaker = f\"Speaker_{speakers[user]}\"\n",
    "                text = utterance['utterance']\n",
    "                segments = split_into_sentences(text)\n",
    "                for segment in segments:\n",
    "                    num_of_rows += 1\n",
    "                    writer.writerow([speaker, segment])\n",
    "                \n",
    "    print(\"Number of rows:\", num_of_rows)\n",
    "\n",
    "# Specify the input JSONL file and the output CSV file\n",
    "jsonl_file = 'qc_with_index.jsonl'\n",
    "csv_file = 'qc_with_token_limit2.csv'\n",
    "\n",
    "# Convert JSONL to CSV with token limit\n",
    "jsonl_to_csv_with_token_limit(jsonl_file, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "qc = pd.read_csv(\"qc_with_index.csv\")\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "TOKEN_LIMIT = 100\n",
    "\n",
    "def is_lengthy(text):\n",
    "    if not text:\n",
    "        return 0\n",
    "    try:\n",
    "        length = len(nlp(text))\n",
    "        if length > TOKEN_LIMIT:\n",
    "            return 1\n",
    "        return 0\n",
    "    except:\n",
    "        print(text)\n",
    "qc['text'] = qc['text'].fillna(\" \")\n",
    "qc['is_lengthy'] = qc['text'].map(lambda x: is_lengthy(x))\n",
    "qc.to_csv(\"qc_with_is_lengthy.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       \n",
       "1             Singapore is ruled by the WEF and not PAP.\n",
       "2      In the 1980-2000, we were ruled by the PAP. No...\n",
       "3      Going by OP logic, much of the world is ruled ...\n",
       "4                                                       \n",
       "                             ...                        \n",
       "360                                                     \n",
       "361    Hydrogen discussions would be more valuable if...\n",
       "362    This sub seems to have \"hydrogen\" up and down ...\n",
       "363    I only have a problem with being mind fucked b...\n",
       "364    Bro did you forgot to take your pills this mor...\n",
       "Name: text, Length: 365, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qc['text'].fillna(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "'''\n",
    "Converts the jsonl file into csv file format. It first splits the text into paragarphs from using \\n\\n, \n",
    "then combines paragraphs if not exceeding token limit. \n",
    "\n",
    "Inputs: path to jsonl file and output path of csv file\n",
    "output: csv file stored at the output path\n",
    "returns: nothing\n",
    "prints: the number of rows of text in the output csv\n",
    "'''\n",
    "def jsonl_to_csv_with_token_limit2(jsonl_file, csv_file, token_limit=100):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def split_into_sentences(text):\n",
    "        # paragraphs = re.split('\\\\n\\\\n|\\\\n>\\s*\\\\n', text)\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        segments = []\n",
    "        cur_segment = \"\"\n",
    "        cur_len = 0\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            k = len(nlp(paragraph))\n",
    "            \n",
    "            if k + cur_len <= token_limit:\n",
    "                cur_segment += '\\n\\n'\n",
    "                cur_segment += paragraph\n",
    "                cur_len += k\n",
    "            else:\n",
    "                cur_len = k\n",
    "                if cur_segment:\n",
    "                    segments.append(cur_segment.strip())\n",
    "                cur_segment = paragraph            \n",
    "\n",
    "        if cur_segment:\n",
    "            segments.append(cur_segment.strip())\n",
    "\n",
    "        return segments\n",
    "\n",
    "    num_of_rows = 0\n",
    "\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as infile, open(csv_file, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow([\"speaker\", \"text\"])\n",
    "        \n",
    "        for i, line in enumerate(infile):\n",
    "            dialogue = json.loads(line.strip())\n",
    "            if not dialogue:\n",
    "                continue\n",
    "            writer.writerow([f\"Dialogue{i}\", \"\"])\n",
    "            speakers2id = {}\n",
    "            nextId = 0\n",
    "            for utterance in dialogue['conversation']:\n",
    "                user = utterance['speaker']\n",
    "                if user not in speakers2id:\n",
    "                    speakers2id[user] = nextId\n",
    "                    nextId += 1\n",
    "                \n",
    "                speaker = f\"Speaker_{speakers2id[user]}\"\n",
    "                text = utterance['utterance']\n",
    "                segments = split_into_sentences(text)\n",
    "                for segment in segments:\n",
    "                    num_of_rows += 1\n",
    "                    writer.writerow([speaker, segment])\n",
    "                \n",
    "    print(\"Number of rows:\", num_of_rows)\n",
    "\n",
    "# # Specify the input JSONL file and the output CSV file\n",
    "# jsonl_file = 'shuffled_CandD.jsonl'\n",
    "# csv_file = 'shuffled_CandD_limit_paragraph.csv'\n",
    "\n",
    "# # Convert JSONL to CSV with token limit\n",
    "# jsonl_to_csv_with_token_limit2(jsonl_file, csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text rows: 865\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import spacy\n",
    "\n",
    "\n",
    "def split_annotated_csv_by_paragraph(input_csv, output_csv, token_limit=100):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def split_into_paragraphs(text):\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        segments = []\n",
    "        cur_segment = \"\"\n",
    "        cur_len = 0\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            k = len(nlp(paragraph))\n",
    "            \n",
    "            if k + cur_len <= token_limit:\n",
    "                cur_segment += '\\n\\n'\n",
    "                cur_segment += paragraph\n",
    "                cur_len += k\n",
    "            else:\n",
    "                cur_len = k\n",
    "                if cur_segment:\n",
    "                    segments.append(cur_segment.strip())\n",
    "                cur_segment = paragraph            \n",
    "\n",
    "        if cur_segment:\n",
    "            segments.append(cur_segment.strip())\n",
    "\n",
    "        return segments\n",
    "\n",
    "    number_of_rows = 0\n",
    "\n",
    "    with open(input_csv, 'r', encoding='utf-8') as infile, open(output_csv, 'w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for row in reader:\n",
    "            # print(row)\n",
    "            # if 'text' not in row:\n",
    "            #     writer.writerow(row)\n",
    "            #     continue\n",
    "            text = row['text']\n",
    "            split_texts = split_into_paragraphs(text)\n",
    "            if len(split_texts) <= 1:\n",
    "                if len(split_texts) == 1:\n",
    "                    number_of_rows += 1\n",
    "                writer.writerow(row)\n",
    "            else:\n",
    "                for split_text in split_texts:\n",
    "                    new_row = {key: (row['speaker'] if key == 'speaker' else '') for key in row}\n",
    "                    new_row['text'] = split_text\n",
    "                    number_of_rows += 1\n",
    "                    writer.writerow(new_row)\n",
    "\n",
    "    print(f'Number of text rows: {number_of_rows}')\n",
    "# Specify the input JSONL file and the output CSV file\n",
    "jsonl_file = 'qc_dialog_annotated_before_split.csv'\n",
    "csv_file = 'qc_with_token_limit_paragraph_annotated.csv'\n",
    "\n",
    "# Convert JSONL to CSV with token limit\n",
    "split_annotated_csv_by_paragraph(jsonl_file, csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "import csv\n",
    "\n",
    "# create an unique id for each conversation by concatenating filename and index in jsonl file\n",
    "# given a json dialog object, returns the id in string\n",
    "def dialog_to_id(dialog):\n",
    "    return dialog[\"filename\"] + dialog[\"index\"]\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# splits the text into a single paragraph by \\n\\n or paragraphs not exceeding token limit\n",
    "def split_into_paragraphs(text, token_limit=100):\n",
    "        paragraphs = text.split(\"\\n\\n\")\n",
    "        segments = []\n",
    "        cur_segment = \"\"\n",
    "        cur_len = 0\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            k = len(nlp(paragraph))\n",
    "            \n",
    "            if k + cur_len <= token_limit:\n",
    "                cur_segment += '\\n\\n'\n",
    "                cur_segment += paragraph\n",
    "                cur_len += k\n",
    "            else:\n",
    "                cur_len = k\n",
    "                if cur_segment:\n",
    "                    segments.append(cur_segment.strip())\n",
    "                cur_segment = paragraph            \n",
    "\n",
    "        if cur_segment:\n",
    "            segments.append(cur_segment.strip())\n",
    "\n",
    "        return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set C: {0, 1, 3, 7, 15}\n",
      "Set D: {2, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 16, 17, 18, 19, 20, 21}\n",
      "Number of convo in E belonging to C: 5\n",
      "Number of convo in E belonging to D: 17\n",
      "Number of utterances from C: 139\n",
      "Number of utterances from D: 166\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set_E_ids = set()\n",
    "\n",
    "# # select top 22 converstations from E, which contains 305 utterances each with 1 paragraph or paragraphs <= 100 tokens\n",
    "# # add these convo id into the set_E_ids\n",
    "# with open('qc_with_index.jsonl', 'r', encoding='utf-8') as infile:\n",
    "#     for i, line in enumerate(infile):\n",
    "#         dialogue = json.loads(line.strip())\n",
    "#         if not dialogue:\n",
    "#             continue\n",
    "#         dial_id = dialog_to_id(dialogue)\n",
    "#         set_E_ids.add(dial_id)\n",
    "\n",
    "# jsonl_file = 'shuffled_CandD.jsonl'\n",
    "# csv_file = 'shuffled_CandD_limit_paragraph.csv'\n",
    "\n",
    "# # Convert JSONL to CSV with token limit\n",
    "# jsonl_to_csv_with_token_limit2(jsonl_file, csv_file)\n",
    "\n",
    "avg_conv_len = 6547 / 600\n",
    "max_conv = 3000 / avg_conv_len\n",
    "max_new_C_conv = max_conv / 6 + 1\n",
    "max_new_D_conv = max_conv * 5 / 6 + 1\n",
    "new_C_convo_size = 0\n",
    "new_D_convo_size = 0\n",
    "C_utterance_size = 0\n",
    "D_utterance_size = 0\n",
    "max_C_utterance_size = 3000 / 6\n",
    "max_D_utterance_size = 3000 * 5 / 6\n",
    "\n",
    "########################## FOR REFERENCES: ############################################\n",
    "## new E will be the top 22 convo of qc_with_index which contains 305 utterances, this will be the new QC dataset\n",
    "## C: 100 conv\n",
    "## D: 500 conv\n",
    "\n",
    "\n",
    "num_of_rows = 0\n",
    "C_index = set()  # index of convo belonging to C\n",
    "D_index = set()  # index of convo belonging to D\n",
    "# get the index of convo in new E that belongs to each dataset\n",
    "with open('qc_with_index.jsonl', 'r', encoding='utf-8') as infile:\n",
    "    for i, line in enumerate(infile):\n",
    "        if i >= 22:  # only selects the top 22 convo for new E\n",
    "            break\n",
    "        dialogue = json.loads(line.strip())\n",
    "        if not dialogue:\n",
    "            continue\n",
    "        if 'singapore' in dialogue['filename']:\n",
    "            C_index.add(i)\n",
    "        else:\n",
    "            D_index.add(i)\n",
    "print(\"Set C:\", C_index)\n",
    "print(\"Set D:\", D_index)\n",
    "# split into 2 sets, annotation and QC\n",
    "# first count the number of utterances in new E that belongs to C and D respectively\n",
    "# This uses the qc_with_token_limit_paragraph (matches the annotated version) and saves the top 22 convo into qc_top_22_convo.csv\n",
    "with open('qc_with_token_limit_paragraph.csv', 'r', encoding='utf-8') as infile, open('qc_top_22_convo.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "    reader = csv.DictReader(infile)\n",
    "    fieldnames = reader.fieldnames\n",
    "    writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    cur_dialog_num = -1\n",
    "    for row in reader:\n",
    "        if row['speaker'] == 'Dialogue22':  # new E only contains the top 22 convo\n",
    "            break\n",
    "        text = row['text']\n",
    "        writer.writerow(row)\n",
    "        \n",
    "            \n",
    "        if not text:\n",
    "            cur_dialog_num += 1\n",
    "            if cur_dialog_num in C_index:\n",
    "                new_C_convo_size += 1\n",
    "            else:\n",
    "                new_D_convo_size += 1\n",
    "        else:\n",
    "            if cur_dialog_num in C_index:\n",
    "                C_utterance_size += 1\n",
    "            else:\n",
    "                D_utterance_size += 1\n",
    "\n",
    "# print(\"Number of utterances in new E:\", num_of_rows)\n",
    "print(\"Number of convo in E belonging to C:\", new_C_convo_size)\n",
    "print(\"Number of convo in E belonging to D:\", new_D_convo_size)\n",
    "print(\"Number of utterances from C:\", C_utterance_size)\n",
    "print(\"Number of utterances from D:\", D_utterance_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of utterances: 3000\n",
      "Number of convo belonging to C: 42\n",
      "Number of convo belonging to D: 248\n",
      "Number of utterances from C: 564\n",
      "Number of utterances from D: 2436\n",
      "C_index: [7, 9, 14, 22, 24, 38, 39, 41, 59, 62, 69, 71, 75, 77, 81, 93, 103, 114, 128, 130, 133, 144, 149, 154, 181, 182, 186, 196, 212, 222, 223, 225, 226, 238, 239, 245, 259]\n",
      "D_index: [0, 1, 2, 3, 4, 5, 6, 8, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 60, 61, 63, 64, 65, 66, 67, 68, 70, 72, 73, 74, 76, 78, 79, 80, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 94, 95, 96, 97, 98, 99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 129, 131, 132, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 145, 146, 147, 148, 150, 151, 152, 153, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 183, 184, 185, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 213, 214, 215, 216, 217, 218, 219, 220, 221, 224, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243, 244, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 260, 261, 262, 263, 264, 265, 266, 267]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# now pick from shuffled_CandD_without_E until we reach max conv size for each group or max utterance size\n",
    "# (top 22 of shuffled_CandD makes up new E, so the above file is obtained by deleting top 22 rows of shuffled_CandD)\n",
    "cur_utterance_size = 305\n",
    "MAX_UTERRANCE_SIZE = 3000\n",
    "\n",
    "C_index = set()\n",
    "D_index = set()\n",
    "with open('shuffled_CandD_without_E.jsonl', 'r', encoding='utf-8') as infile, open('new_CandD_energy.csv', 'w', newline='', encoding='utf-8') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow([\"speaker\", \"text\", \"emotion\", \"sentiment\"])\n",
    "    \n",
    "    for i, line in enumerate(infile):\n",
    "        if cur_utterance_size >= 3000 or (new_C_convo_size >= max_new_C_conv and new_D_convo_size >= max_new_D_conv):\n",
    "            break\n",
    "        dialogue = json.loads(line.strip())\n",
    "        if not dialogue:\n",
    "            continue\n",
    "        \n",
    "        can_be_added = None\n",
    "        if 'singapore' in dialogue['filename'] and new_C_convo_size < max_new_C_conv:\n",
    "            C_index.add(i)\n",
    "            new_C_convo_size += 1\n",
    "            can_be_added = 'C'\n",
    "        elif 'singapore' not in dialogue['filename'] and new_D_convo_size < max_D_utterance_size:\n",
    "            D_index.add(i)\n",
    "            new_D_convo_size += 1 \n",
    "            can_be_added = 'D'\n",
    "        \n",
    "        if not can_be_added:\n",
    "            continue\n",
    "\n",
    "        writer.writerow([f\"Dialogue{i}\", \"\"])\n",
    "        speakers2id = {}\n",
    "        nextId = 0\n",
    "        for utterance in dialogue['conversation']:\n",
    "            user = utterance['speaker']\n",
    "            if user not in speakers2id:\n",
    "                speakers2id[user] = nextId\n",
    "                nextId += 1\n",
    "            \n",
    "            speaker = f\"Speaker_{speakers2id[user]}\"\n",
    "            text = utterance['utterance']\n",
    "            segments = split_into_paragraphs(text)\n",
    "            for segment in segments:\n",
    "                cur_utterance_size += 1\n",
    "                if can_be_added == 'C':\n",
    "                    C_utterance_size += 1\n",
    "                else:\n",
    "                    D_utterance_size += 1\n",
    "                writer.writerow([speaker, segment, \"\", \"\"])\n",
    "                \n",
    "print(\"Number of utterances:\", cur_utterance_size)\n",
    "print(\"Number of convo belonging to C:\", new_C_convo_size)\n",
    "print(\"Number of convo belonging to D:\", new_D_convo_size)\n",
    "print(\"Number of utterances from C:\", C_utterance_size)\n",
    "print(\"Number of utterances from D:\", D_utterance_size)\n",
    "print(\"C_index:\", sorted(list(C_index)))\n",
    "print(\"D_index:\", sorted(list(D_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C convo size: 43\n",
      "D convo size 248\n",
      "C utterance size 571\n",
      "D utterance size 2434\n"
     ]
    }
   ],
   "source": [
    "cur_utterance_size = 0\n",
    "MAX_UTERRANCE_SIZE = 3000\n",
    "\n",
    "new_C_convo_size = 0\n",
    "new_D_convo_size = 0\n",
    "cur_utterance_size = 0\n",
    "C_utterance_size = 0\n",
    "D_utterance_size = 0\n",
    "E_from_C = 0\n",
    "E_from_D = 0\n",
    "with open('shuffled_CandD.jsonl', 'r', encoding='utf-8') as infile, open('new_C.jsonl', 'w', encoding='utf-8') as new_C, open('new_D.jsonl', 'w', encoding='utf-8') as new_D:\n",
    "    for i, line in enumerate(infile):\n",
    "\n",
    "        if cur_utterance_size >= 3000 or (new_C_convo_size >= max_new_C_conv and new_D_convo_size >= max_new_D_conv):\n",
    "            break\n",
    "        dialogue = json.loads(line.strip())\n",
    "        if not dialogue:\n",
    "            continue\n",
    "        \n",
    "        add_to = None\n",
    "        if 'singapore' in dialogue['filename'] and new_C_convo_size < max_new_C_conv:\n",
    "            # C_index.add(i)\n",
    "            new_C_convo_size += 1\n",
    "            new_C.write(line)\n",
    "            add_to = 'C'\n",
    "        elif 'singapore' not in dialogue['filename'] and new_D_convo_size < max_D_utterance_size:\n",
    "            # D_index.add(i)\n",
    "            new_D_convo_size += 1 \n",
    "            new_D.write(line)\n",
    "            add_to = 'D'\n",
    "\n",
    "        for utterance in dialogue['conversation']:\n",
    "            text = utterance['utterance']\n",
    "            segments = split_into_paragraphs(text)\n",
    "            for segment in segments:\n",
    "                cur_utterance_size += 1\n",
    "                if add_to == 'C':\n",
    "                    if i < 22:\n",
    "                        E_from_C += 1\n",
    "                    C_utterance_size += 1\n",
    "                else:\n",
    "                    if i < 22:\n",
    "                        E_from_D += 1\n",
    "                    D_utterance_size += 1\n",
    "            \n",
    "print(\"C convo size:\", new_C_convo_size)\n",
    "print(\"D convo size\", new_D_convo_size)\n",
    "print(\"C utterance size\", C_utterance_size)\n",
    "print(\"D utterance size\", D_utterance_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffled lines written to shuffled_drone_reddit.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# Combine 100 conv (C) and 500 conv (D) and shuffle to get the original pool of qc\n",
    "\n",
    "# Paths for input and output files\n",
    "input_file_path = 'drone_reddit.jsonl'\n",
    "output_file_path = 'shuffled_drone_reddit.jsonl'\n",
    "\n",
    "# Read lines from the JSONL file\n",
    "with open(input_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# Shuffle the lines\n",
    "random.Random(14).shuffle(lines)\n",
    "\n",
    "# Write the shuffled lines to a new file\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.writelines(lines)\n",
    "\n",
    "print(f'Shuffled lines written to {output_file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reverse engineer to get back the non annotated csv from the annotated csv of qc_split_by_paragraph\n",
    "# this is due to minor discrepencies between the annotated dialg and the jsonl file\n",
    "\n",
    "df = pd.read_csv('qc_with_token_limit_paragraph_annotated.csv')\n",
    "df.drop(columns=['remarks','emotion-ly','sentiment-ly','remarks-ly'], inplace=True)\n",
    "columns_to_clear = ['emotion', 'sentiment']\n",
    "\n",
    "df[columns_to_clear] = \"\"\n",
    "\n",
    "#remove the mistakes of empty rows in previous csv file\n",
    "# Define the filter condition\n",
    "condition = ~((df['speaker'].str.contains('speaker', case=False, na=False)) & (df['text'].isna() | df['text'].eq('')))\n",
    "\n",
    "# Apply the filter to the DataFrame\n",
    "cleaned_df = df[condition]\n",
    "\n",
    "cleaned_df.to_csv('qc_with_token_limit_paragraph.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 0 saved to energy0.csv\n",
      "Dataset 1 saved to energy1.csv\n",
      "Dataset 2 saved to energy2.csv\n",
      "Dataset 3 saved to energy3.csv\n",
      "Dataset 4 saved to energy4.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files into DataFrames\n",
    "df_E = pd.read_csv('qc_top_22_convo.csv')\n",
    "df_D = pd.read_csv('new_CandD_energy.csv')\n",
    "\n",
    "def split_dialogues(df, n_chunks):\n",
    "    dialogues = df[df['speaker'].str.contains('Dialogue', na=False)]\n",
    "    dialogue_indices = dialogues.index.tolist()\n",
    "    chunk_size = len(dialogue_indices) // n_chunks\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(n_chunks):\n",
    "        start_idx = dialogue_indices[i * chunk_size]\n",
    "        if i == n_chunks - 1:\n",
    "            end_idx = df.index[-1] + 1\n",
    "        else:\n",
    "            end_idx = dialogue_indices[(i + 1) * chunk_size]\n",
    "        chunks.append(df[start_idx:end_idx])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "dialogue_counter = 0\n",
    "\n",
    "def relabel_dialogues(df):\n",
    "    global dialogue_counter\n",
    "    for idx, row in df.iterrows():\n",
    "        if 'Dialogue' in str(row['speaker']):\n",
    "            df.at[idx, 'speaker'] = f'Dialogue{dialogue_counter}'\n",
    "            dialogue_counter += 1\n",
    "    return df\n",
    "\n",
    "# Split the dialogues in both DataFrames\n",
    "chunks_E = split_dialogues(df_E, 5)\n",
    "chunks_D = split_dialogues(df_D, 5)\n",
    "\n",
    "# Combine the chunks from both DataFrames\n",
    "combined_chunks = [pd.concat([chunks_E[i], chunks_D[i]], ignore_index=True) for i in range(5)]\n",
    "\n",
    "# Relabel the dialogues in each chunk\n",
    "combined_chunks = [relabel_dialogues(chunk) for chunk in combined_chunks]\n",
    "\n",
    "# Save the datasets to new CSV files\n",
    "for i, dataset in enumerate(combined_chunks):\n",
    "    dataset.to_csv(f'energy{i}.csv', index=False)\n",
    "    print(f'Dataset {i} saved to energy{i}.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 721\n"
     ]
    }
   ],
   "source": [
    "jsonl_to_csv_with_token_limit2('shuffled_drone_reddit.jsonl', 'shuffled_drone_reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of conv from A: 14\n",
      "Number of convo from B: 64\n",
      "Number of utterances from A: 166\n",
      "Number of utterances from B: 336\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"data/drone/shuffled_drone_reddit.jsonl\", \"r\") as infile:\n",
    "    counter = 1\n",
    "    convo_from_A = 0\n",
    "    convo_from_B = 0\n",
    "    utterance_from_A = 0\n",
    "    utterance_from_B = 0\n",
    "    for line in infile:\n",
    "        if counter > 81:\n",
    "            break\n",
    "        counter += 1\n",
    "        dialogue = json.loads(line)\n",
    "        if \"singapore\" in dialogue[\"filename\"]:\n",
    "            convo_from_A += 1\n",
    "        else:\n",
    "            convo_from_B += 1\n",
    "\n",
    "        for utterance in dialogue['conversation']:\n",
    "            text = utterance['utterance']\n",
    "            segments = split_into_paragraphs(text)\n",
    "            if \"singapore\" in dialogue[\"filename\"]:\n",
    "                utterance_from_A += len(segments)\n",
    "            else:\n",
    "                utterance_from_B += len(segments)\n",
    "\n",
    "    print(\"Number of conv from A:\", convo_from_A)\n",
    "    print(\"Number of convo from B:\", convo_from_B)\n",
    "    print(\"Number of utterances from A:\", utterance_from_A)\n",
    "    print(\"Number of utterances from B:\", utterance_from_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 4/4 [00:00<00:00, 206.20ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 351.28ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 241.13ba/s]\n",
      "Downloading data: 100%|██████████| 3.78M/3.78M [00:08<00:00, 431kB/s]\n",
      "Downloading data: 100%|██████████| 901k/901k [00:01<00:00, 790kB/s]\n",
      "Downloading data: 100%|██████████| 167k/167k [00:01<00:00, 148kB/s]\n",
      "Generating train split: 100%|██████████| 45615/45615 [00:00<00:00, 822022.10 examples/s]\n",
      "Generating test split: 100%|██████████| 12284/12284 [00:00<00:00, 881817.46 examples/s]\n",
      "Generating validation split: 100%|██████████| 2000/2000 [00:00<00:00, 508338.87 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 46/46 [00:00<00:00, 324.08ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 394.35ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 416.87ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1177979"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"emotion\")\n",
    "ds['train'].to_csv(\"data/tweet_eval/emotion/train.csv\")\n",
    "ds['validation'].to_csv(\"data/tweet_eval/emotion/validation.csv\")\n",
    "ds['test'].to_csv(\"data/tweet_eval/emotion/test.csv\")\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")\n",
    "ds['train'].to_csv(\"data/tweet_eval/sentiment/train.csv\")\n",
    "ds['validation'].to_csv(\"data/tweet_eval/sentiment/validation.csv\")\n",
    "ds['test'].to_csv(\"data/tweet_eval/sentiment/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "int_to_emotion = [\"anger\", \"joy\", \"optimism\", \"sadness\"]\n",
    "emotion_to_int = {\"anger\": 0, \"joy\": 1, \"optimism\": 2, \"sadness\": 3}\n",
    "int_to_sentiment = [\"negative\", \"neutral\", \"positive\"]\n",
    "sentiemnt_to_int = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "ds = load_dataset(\"cardiffnlp/tweet_eval\", \"sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 45615/45615 [00:00<00:00, 48488.98 examples/s]\n",
      "Map: 100%|██████████| 12284/12284 [00:00<00:00, 38582.96 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 48011.45 examples/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 46/46 [00:00<00:00, 420.31ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 359.01ba/s]\n",
      "Creating CSV from Arrow format: 100%|██████████| 13/13 [00:00<00:00, 410.74ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1222366"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def process_label(example):\n",
    "    example['str_label'] = int_to_emotion[example['label']]\n",
    "    return example\n",
    "\n",
    "ds = ds.map(process_label, batched=False)\n",
    "ds = ds.remove_columns([\"label\"])\n",
    "ds = ds.rename_column(\"str_label\", \"label\")\n",
    "ds['train'].to_csv(\"data/tweet_eval/sentiment/train.csv\")\n",
    "ds['validation'].to_csv(\"data/tweet_eval/sentiment/validation.csv\")\n",
    "ds['test'].to_csv(\"data/tweet_eval/sentiment/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'language', 'emotion_sw', 'sentiment', 'overall_sentiment_sw', 'emotion_sn', 'sentiment.1', 'overall_sentiment_sn', 'emotion_do', 'sentiment.2', 'overall_sentiment_do', 'Inter-annotator Emotion Agreement', 'Inter-annotator Sentiment Agreement', 'Fully Agreed Emotion', 'Fully Agreed Sentiment', 'Unnamed: 15', 'voted_emotion', 'voted_sentiment', 'Manually Labelled emotion:', 'Manually labelled sentiment'],\n",
       "    num_rows: 2501\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "load_dataset('csv', data_files=\"data/drone/responses/all_tweets_full_responses.csv\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'CAAM will continue to provide its full support to the rapidly growing drone technology in the country. We are happy to join the group and witnessed the first drone delivery demonstration held at Cyberjaya today.  #CAAM  #TeamCAAM  #CAAMalaysia  #UrbanDroneDeliverySanboxProject  [URL]', 'language': 'en', 'emotion_sw': 'happiness', 'sentiment': '', 'overall_sentiment_sw': 'positive', 'emotion_sn': 'other', 'overall_sentiment_sn': 'neutral', 'emotion_do': 'happiness', 'overall_sentiment_do': 'positive', 'Inter-annotator Emotion Agreement': 'TRUE', 'Inter-annotator Sentiment Agreement': 'TRUE', 'Fully Agreed Emotion': 'FALSE', 'Fully Agreed Sentiment': 'FALSE', '': '', 'voted_emotion': 'happiness', 'voted_sentiment': 'positive', 'Manually Labelled emotion:': '', 'Manually labelled sentiment': ''}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "data_file = \"data/drone/responses/all_tweets_full_responses.csv\"\n",
    "with open(data_file, 'r', newline='') as infile:\n",
    "    csv_reader = csv.DictReader(infile)\n",
    "    rows = list(csv_reader)\n",
    "    print(rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
