{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from statsmodels.stats import inter_rater as irr\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 15\n",
      "count          0.0\n",
      "mean           NaN\n",
      "std            NaN\n",
      "min            NaN\n",
      "25%            NaN\n",
      "50%            NaN\n",
      "75%            NaN\n",
      "max            NaN\n",
      "Conversion to JSON Lines completed.\n"
     ]
    }
   ],
   "source": [
    "### Convert the annotated csv file into jsonline file in the required format  \n",
    "### Used for tweets\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = \"data/drone/responses/all_tweets_full_responses.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "# print(df.describe())\n",
    "output_path = \"data/drone/responses/all_tweets_annotated.jsonl\"\n",
    "\n",
    "# Open a file to write the JSON Lines\n",
    "with open(output_path, \"w\") as jsonl_file:\n",
    "    # Iterate through each row in the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Create the JSON structure for each row\n",
    "        json_data = {\n",
    "            \"text\": row['text'],\n",
    "            \"emotion\": row['voted_emotion'],\n",
    "            \"sentiment\": row['voted_sentiment'],\n",
    "            \"annotations\": {\n",
    "                \"emotion\": [row['emotion_sw'], row['emotion_sn'], row['emotion_do']],\n",
    "                \"sentiment\": [row['overall_sentiment_sw'], row['overall_sentiment_sn'], row['overall_sentiment_do']]\n",
    "            }\n",
    "        }\n",
    "        # Write the JSON data as a line in the JSON Lines file\n",
    "        jsonl_file.write(json.dumps(json_data) + \"\\n\")\n",
    "\n",
    "print(\"Conversion to JSON Lines completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion to combined JSON Lines completed.\n"
     ]
    }
   ],
   "source": [
    "### Converts csv file to required jsonline format\n",
    "### Used for reddit\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = \"data/energy/responses/full_energy_annotated.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Load the existing JSON Lines file\n",
    "jsonl_file = \"data/energy/shuffled_CandD.jsonl\"   #only use up to 290 convo for energy\n",
    "with open(jsonl_file, \"r\") as file:\n",
    "    jsonl_data = [json.loads(line) for line in file]\n",
    "\n",
    "# Function to determine agreement level\n",
    "def get_annotations(row):\n",
    "    return {\n",
    "        \"emotion\": [row['emotion_sw'], row['emotion_sn'], row['emotion_do']],\n",
    "        \"sentiment\": [row['overall_sentiment_sw'], row['overall_sentiment_sn'], row['overall_sentiment_do']]\n",
    "    }\n",
    "\n",
    "grouped = df.groupby((df['speaker'].str.startswith('Dialogue')).cumsum())\n",
    "# Write the combined data to a new JSON Lines file\n",
    "output_jsonl_file = \"full_energy_annotated.jsonl\"\n",
    "\n",
    "# Iterate through each conversation in jsonl_data and corresponding dialogue group in the dataframe\n",
    "with open(output_jsonl_file, \"w\") as output_file:\n",
    "    for i, (name, group) in enumerate(grouped):\n",
    "        if i >= len(jsonl_data):\n",
    "            break\n",
    "        # print(f\"Convo {i}\")\n",
    "        jsonl_conversation = jsonl_data[i]\n",
    "        first_utterance = jsonl_conversation['conversation'][0]['utterance']\n",
    "        # Create the conversation list\n",
    "        conversation_list = []\n",
    "        for j, row in group.iterrows():\n",
    "            # check if first utterance is the same\n",
    "            if j == 1 and row['text'] != first_utterance:\n",
    "                raise AssertionError(f\"first utterance of convo {i} does not match!\")\n",
    "            # print(f\"row {_} of dialogue {i}\")\n",
    "            if row['speaker'].startswith(\"Dialogue\"):\n",
    "                continue\n",
    "\n",
    "            conversation_list.append({\n",
    "                \"utterance\": row['text'],\n",
    "                \"speaker\": row['speaker'],\n",
    "                \"emotion\": row['voted_emotion'],\n",
    "                \"sentiment\": row['voted_sentiment'],\n",
    "                \"annotations\": get_annotations(row)\n",
    "            })\n",
    "\n",
    "        # Update the conversation in jsonl_conversation\n",
    "        jsonl_conversation['conversation'] = conversation_list\n",
    "        \n",
    "        # Write the updated conversation to the output file\n",
    "        output_file.write(json.dumps(jsonl_conversation) + \"\\n\")\n",
    "\n",
    "print(\"Conversion to combined JSON Lines completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for confusion matrix VV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Batch1_tweets_responses.csv\")\n",
    "\n",
    "# confusion_matrix(df['golden_sentiment'], df['sentiment_sw'], labels=['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'other'])\n",
    "confusion_matrix(df['golden_sentiment'], df['overall_sentiment_do'], labels=['positive', 'neutral', 'negative'])\n",
    "confusion_matrix(df['golden_sentiment'], df['sentiment_sw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3340581531758013\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/energy/responses/full_energy_annotated.csv\")\n",
    "df = df[~df['text'].isna()]\n",
    "\n",
    "emotions = df[['emotion_sw', 'emotion_sn', 'emotion_do']]\n",
    "agg_emotions = irr.aggregate_raters(emotions)\n",
    "print(irr.fleiss_kappa(agg_emotions[0], method='fleiss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4690062436824931\n"
     ]
    }
   ],
   "source": [
    "sentiments = df[['overall_sentiment_sw', 'overall_sentiment_sn', 'overall_sentiment_do']]\n",
    "agg_sentiments = irr.aggregate_raters(sentiments)\n",
    "print(irr.fleiss_kappa(agg_sentiments[0], method='fleiss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall_sentiment_sw</th>\n",
       "      <th>overall_sentiment_sn</th>\n",
       "      <th>overall_sentiment_do</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>neutral</td>\n",
       "      <td>NaN</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall_sentiment_sw overall_sentiment_sn overall_sentiment_do\n",
       "71              neutral                  NaN              neutral"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[sentiments['overall_sentiment_sn'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/chaoming/emotion_classification\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix)\n",
    "\n",
    "datafile = \"data/drone/responses/all_tweets_full_responses.csv\"\n",
    "df = pd.read_csv(datafile, encoding=\"utf-8\", encoding_errors=\"replace\")\n",
    "\n",
    "emotions = [\"happiness\", \"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"other\"]\n",
    "sentiments = [\"positive\", \"negative\", \"neutral\"]\n",
    "sent_mapping = {'positive': 2, 'neutral': 1, 'negative': 0}\n",
    "emotion_mapping = {\"happiness\":0, \"anger\":1, \"disgust\":2, \"fear\":3, \"sadness\":4, \"surprise\":5, \"other\":6}\n",
    "\n",
    "df[\"emotion\"] = df[\"voted_emotion\"]\n",
    "df[\"sentiment\"] = df[\"voted_sentiment\"]\n",
    "\n",
    "# pred_path = \"output/drone/local_llama3_8B/test/masked_all_tweets_llama3.csv\"\n",
    "# preds_df = pd.read_csv(pred_path)\n",
    "# preds_df[\"emotion\"] = preds_df[\"llama3_emotion\"]\n",
    "# preds_df[\"sentiment\"] = preds_df[\"llama3_sentiment\"]\n",
    "\n",
    "# def map_func(x):\n",
    "#     return mapping.get(x, 1)\n",
    "\n",
    "def evaluate(y_true, y_pred, labels):\n",
    "    \n",
    "    y_true = y_true.tolist()\n",
    "    y_pred = y_pred.tolist()\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.5f}')\n",
    "    \n",
    "    # Generate accuracy report\n",
    "    unique_labels = set(y_true)  # Get unique labels\n",
    "    \n",
    "    for label in labels:\n",
    "        label_indices = [i for i in range(len(y_true)) \n",
    "                         if y_true[i] == label]\n",
    "        label_y_true = [y_true[i] for i in label_indices]\n",
    "        label_y_pred = [y_pred[i] for i in label_indices]\n",
    "        accuracy = accuracy_score(label_y_true, label_y_pred)\n",
    "        print(f'Accuracy for label {label}: {accuracy:.5f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred, digits=5)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels)\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "llama3_emotion\n",
       "other        1355\n",
       "anger         540\n",
       "happiness     317\n",
       "surprise      109\n",
       "fear           63\n",
       "sadness        34\n",
       "disgust        15\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_df[\"llama3_emotion\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.58137\n",
      "Accuracy for label happiness: 0.92000\n",
      "Accuracy for label anger: 0.97872\n",
      "Accuracy for label disgust: 0.01471\n",
      "Accuracy for label fear: 0.23077\n",
      "Accuracy for label sadness: 0.33333\n",
      "Accuracy for label surprise: 0.30508\n",
      "Accuracy for label other: 0.59465\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger    0.08519   0.97872   0.15673        47\n",
      "     disgust    0.06667   0.01471   0.02410        68\n",
      "        fear    0.09524   0.23077   0.13483        26\n",
      "   happiness    0.14511   0.92000   0.25068        50\n",
      "         nan    0.00000   0.00000   0.00000         0\n",
      "       other    0.98524   0.59465   0.74167      2245\n",
      "     sadness    0.05882   0.33333   0.10000         6\n",
      "    surprise    0.16514   0.30508   0.21429        59\n",
      "\n",
      "    accuracy                        0.58137      2501\n",
      "   macro avg    0.20018   0.42216   0.20279      2501\n",
      "weighted avg    0.89573   0.58137   0.68106      2501\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  46    1    0    0    1    0    2]\n",
      " [   0   46    0    0    1    0    0]\n",
      " [   2   57    1    1    2    0    5]\n",
      " [   0   15    1    6    2    0    2]\n",
      " [   1    2    0    0    2    0    1]\n",
      " [  10   18    1    1    0   18   10]\n",
      " [ 258  401   12   55   26   91 1335]]\n",
      "Accuracy: 0.65054\n",
      "Accuracy for label positive: 0.90338\n",
      "Accuracy for label negative: 0.81235\n",
      "Accuracy for label neutral: 0.58814\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         nan    0.00000   0.00000   0.00000         0\n",
      "    negative    0.50152   0.81235   0.62017       405\n",
      "     neutral    0.95120   0.58814   0.72686      1889\n",
      "    positive    0.29356   0.90338   0.44313       207\n",
      "\n",
      "    accuracy                        0.65054      2501\n",
      "   macro avg    0.43657   0.57597   0.44754      2501\n",
      "weighted avg    0.82395   0.65054   0.68610      2501\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 187    8   11]\n",
      " [  29  329   46]\n",
      " [ 421  319 1111]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chaoming/anaconda3/envs/emotion_classification/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/chaoming/anaconda3/envs/emotion_classification/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/chaoming/anaconda3/envs/emotion_classification/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/chaoming/anaconda3/envs/emotion_classification/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/chaoming/anaconda3/envs/emotion_classification/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/chaoming/anaconda3/envs/emotion_classification/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# print(df['emotion'].describe())\n",
    "evaluate(df['emotion'], preds_df['emotion'], emotions)\n",
    "evaluate(df['sentiment'], preds_df['sentiment'], sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import evaluate\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "emotion_labels = [\"anger\", \"joy\", \"optimism\", \"sadness\"]\n",
    "sentiment_labels = [\"negative\", \"neutral\", \"positive\"]\n",
    "pred_path = Path(\"output/tweet_eval_emotion/tweet_eval-emotion-1.0-lora/max_new_toks=30-temp=0.1-rep_pen=1.2-combine_prompts=False-few_shots=True\")\n",
    "golden = pd.read_csv(\"data/tweet_eval/emotion/test.csv\")\n",
    "pred_file = Path(pred_path / \"predictions.csv\")\n",
    "predictions = pd.read_csv(pred_file)\n",
    "\n",
    "#### TODO: PUT PRINTED RESULTS IN THE OUTPUT_PATH log file########\n",
    "emotion_report = evaluate(golden[\"label\"],predictions[\"emotion\"], emotion_labels)\n",
    "# sentiment_report = evaluate(golden[\"label\"],predictions[\"sentiment\"], sentiment_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(pred_path / \"emotion_report.json\", \"w\") as f2:\n",
    "    json.dump(emotion_report, f2, indent=2, ensure_ascii=False)\n",
    "\n",
    "# with open(output_path / \"sentiment_report.json\", \"w\") as f2:\n",
    "#     json.dump(sentiment_report, f2, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
